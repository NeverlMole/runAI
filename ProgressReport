1. Model

	General Model
	
		In our project, we adopt the common enviroment-agent model. We build a simulator for simulating our running robot based on Mujoco physical engine and an agent for decision making. At each time step, we get the current state s of the simulator and ask the agent to choose an action a depends on s. Then, let the simulator change the action to a and do one step simulation.
		For the training part, after do the simulation, we get the state of simulator again and use it as the next state s'. We also get an reward r for s' from simulaltor. Then we use s, a, s', r to update the inner parameter of the agent.
		
	State Model
	
		Since our project is a robotic problem, the complete state space is continues and high dimensional. For our running robot, the data we could get from Mujoco physical engine is vast, including the position and verlocity of each leg and body, and the angle and the angular velocity of each joint. After analysis, we choose 10 features got from physical engine and use them to represent a state. They are a_r angle of total body rotation, a_b angle of total body bend, a_{ul}, a_{ur}, v_{ul}, v_{ur} which represent two angles and two angular velocities of the joints between the body and two up-legs, a_{dl], a_{dr}, v_{dl}, v_{dr} for the joints between up-legs and low-legs. 
		Denote the angle space by A and angular velocity space by V, and we have
			a_{dl},a_{dr}...\in A v_{dl}..\in V.
		
		Denote the total state space by \mathcal{S}, and we have
			\mathcal{S} = A^6 \times V^4
			
		For angle space A, we embed it into R and limit the angle from -\pi to \pi, so $A=[-\pi,\pi]$.
		For angular velocity space V, we embed it into R too and limit the value got by simulator from -5 to 5, so $V=[-5,5]$.
		
	Initial and End State
	
		At initial state, the robot is hund in the air a little bit above the groud, and the whole body is steight. Let the axis the robot faced initially be x-axis, the height be the z-axis and the rest be the y-axis.
		
		For the end states, if robot's height is too low or the angel between x-axis and the direction the robot faced is too large, the robot hard to go streight again. Therefore, we regard those states as end states.
		
		
	Action Model
		
		Since our project is a robotic problem, the action space is supposed to be continues too, but to simplify the task for current stage, we use finite action space and there are only 5 actions. Denote the total action space by \mathcal{A}, and we have
			\mathcal{A}=\Set{'Q','W','O','P','S'}
		where 'S' represents no force and each of the rest represents different kinds of constant force apply to the joints, and the spercific desciption is in the project proposal.
		
	Reward 
		For each state, we simply use the velocity along the x-axis be reward. Toward front, reward is positive.
		

2. Algorithm

	Offline version
	
		The above algorithm is online, since the be
